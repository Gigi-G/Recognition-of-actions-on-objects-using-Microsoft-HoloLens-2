{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57ba0a3",
   "metadata": {
    "id": "b57ba0a3"
   },
   "source": [
    "# LSTM - 2.1\n",
    "\n",
    "Terza implementazione di LSTM con sovrapposizione delle finestre temporali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d826350",
   "metadata": {
    "id": "2d826350"
   },
   "source": [
    "## Caricamento del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac63e0",
   "metadata": {
    "id": "5cac63e0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "main_folder = \"../data\"\n",
    "\n",
    "folder = \"./runs/LSTM_1\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(folder, ignore_errors=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "writer = SummaryWriter(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbaa788",
   "metadata": {
    "id": "acbaa788"
   },
   "source": [
    "## Train - Validation - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"No_action\",\n",
    "    \"Prendi\",\n",
    "    \"Rilascia\",\n",
    "    \"Premi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affb0f7",
   "metadata": {
    "id": "8affb0f7"
   },
   "outputs": [],
   "source": [
    "class2idx = {\n",
    "    \"No_action\": 0,\n",
    "    \"Prendi\": 1,\n",
    "    \"Rilascia\": 2,\n",
    "    \"Premi\": 3\n",
    "}\n",
    "\n",
    "idx2class = {v: k for k, v in class2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed37c9",
   "metadata": {
    "id": "49ed37c9"
   },
   "outputs": [],
   "source": [
    "def join_csv(folder_set, filename):\n",
    "    csv:list = []\n",
    "\n",
    "    for file in glob.glob(folder_set + \"/*.csv\"):\n",
    "        csv.append(file)\n",
    "\n",
    "    columns:bool = True\n",
    "    with open(filename, \"w\") as f:\n",
    "        for fcsv in csv:\n",
    "            with open(fcsv, \"r\") as fc:\n",
    "                if columns:\n",
    "                    f.writelines(fc.readlines())\n",
    "                    columns = False\n",
    "                fc.readline()\n",
    "                f.writelines(fc.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yoh1ul_pHjc5",
   "metadata": {
    "id": "Yoh1ul_pHjc5"
   },
   "outputs": [],
   "source": [
    "def create_set(folder_set, filename) -> list:\n",
    "    csv:list = []\n",
    "    for file in glob.glob(folder_set + \"/*.csv\"):\n",
    "        csv.append(file)\n",
    "    columns:bool = True\n",
    "    colname = []\n",
    "    data = []\n",
    "    target = []\n",
    "    for fcsv in csv:\n",
    "        data_video = []\n",
    "        target_video = []\n",
    "        with open(fcsv, \"r\") as fc:\n",
    "            if columns:\n",
    "                colname = [(name) for name in fc.readline().split(\",\")]\n",
    "                colname[-1] = colname[-1][:-1]\n",
    "                columns = False\n",
    "            else:\n",
    "                fc.readline()\n",
    "            for row in fc.readlines():\n",
    "                split_row = row.split(\",\")\n",
    "                data_video.append(split_row[:-1])\n",
    "                target_video.append(class2idx[split_row[-1][:-1]])\n",
    "        data.append(np.array(data_video).astype(float))\n",
    "        target.append(np.array(target_video).astype(int))\n",
    "    return (np.array(data, dtype=object), np.array(target, dtype=object), colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-znJCwBvJBZK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-znJCwBvJBZK",
    "outputId": "d587f561-d7ec-44c7-d3b2-d6a99535b586"
   },
   "outputs": [],
   "source": [
    "folder_set = [[main_folder + \"/train_set\", main_folder + \"/train.csv\"], [main_folder + \"/test_set\", main_folder + \"/test.csv\"], [main_folder + \"/val_set\", main_folder + \"/val.csv\"]]\n",
    "\n",
    "train_array, train_label_array, colname = create_set(folder_set[0][0], folder_set[0][1])\n",
    "test_array, test_label_array, colname = create_set(folder_set[1][0], folder_set[1][1])\n",
    "val_array, val_label_array, colname = create_set(folder_set[2][0], folder_set[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68198fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_array.shape, train_label_array.shape, test_array.shape, test_label_array.shape, val_array.shape, val_label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd772ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem, label in zip(train_array, train_label_array):\n",
    "    print(elem.shape, label.shape)\n",
    "    print(type(elem), type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca6d5f",
   "metadata": {
    "id": "3eca6d5f"
   },
   "outputs": [],
   "source": [
    "folder_set = [[main_folder + \"/train_set\", main_folder + \"/train.csv\"], [main_folder + \"/test_set\", main_folder + \"/test.csv\"], [main_folder + \"/val_set\", main_folder + \"/val.csv\"]]\n",
    "\n",
    "for f_set, filename in folder_set:\n",
    "    join_csv(f_set, filename)\n",
    "    \n",
    "train = pd.read_csv(main_folder + \"/train.csv\")\n",
    "test = pd.read_csv(main_folder + \"/test.csv\")\n",
    "val = pd.read_csv(main_folder + \"/val.csv\")\n",
    "\n",
    "train['TARGET'].replace(class2idx, inplace=True)\n",
    "test['TARGET'].replace(class2idx, inplace=True)\n",
    "val['TARGET'].replace(class2idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38725f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "7a38725f",
    "outputId": "277f27e8-0baf-4e19-fa2a-578dc71deade"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27cc3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "4a27cc3b",
    "outputId": "4d5d44b3-9ed0-475b-995a-1228f9356d7a"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522085c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "7522085c",
    "outputId": "30033aea-7275-464a-a69e-4b42101b4db9"
   },
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e196b6c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e196b6c0",
    "outputId": "1178bdd2-d79b-4211-e8eb-96101ac9716c"
   },
   "outputs": [],
   "source": [
    "print(train.shape[0], test.shape[0], val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ea305",
   "metadata": {
    "id": "da4ea305"
   },
   "source": [
    "Si separano le colonne delle features dall'etichetta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c6d06b",
   "metadata": {
    "id": "d1c6d06b"
   },
   "outputs": [],
   "source": [
    "train_label = train[\"TARGET\"]\n",
    "test_label = test[\"TARGET\"]\n",
    "val_label = val[\"TARGET\"]\n",
    "\n",
    "del train[\"TARGET\"]\n",
    "del test[\"TARGET\"]\n",
    "del val[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owUiLKf4H9jh",
   "metadata": {
    "id": "owUiLKf4H9jh"
   },
   "source": [
    "## Visualizzazione della distribuzione delle classi in Train, Val e Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NvPdymyAH07E",
   "metadata": {
    "id": "NvPdymyAH07E"
   },
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        \"No_action\": 0,\n",
    "        \"Prendi\": 0,\n",
    "        \"Rilascia\": 0,\n",
    "        \"Premi\": 0,\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['No_action'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['Prendi'] += 1\n",
    "        elif i == 2: \n",
    "            count_dict['Rilascia'] += 1\n",
    "        elif i == 3: \n",
    "            count_dict['Premi'] += 1            \n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "            \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mdj_1catIJPD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "mdj_1catIJPD",
    "outputId": "088c5bea-3744-4db5-8234-dea09fe02719"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(25,7))\n",
    "# Train\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(train_label)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Class Distribution in Train Set')\n",
    "# Validation\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(val_label)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[1]).set_title('Class Distribution in Val Set')\n",
    "# Test\n",
    "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(test_label)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[2]).set_title('Class Distribution in Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef0ed3",
   "metadata": {
    "id": "4fef0ed3"
   },
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948a1a9",
   "metadata": {
    "id": "0948a1a9"
   },
   "outputs": [],
   "source": [
    "window_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126e325",
   "metadata": {
    "id": "0126e325"
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, target, window_size = 2):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - window_size):\n",
    "        dataX.append(dataset[i:(i + window_size)])\n",
    "        dataY.append(target[i:(i + window_size + 1)].values[0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b12e8",
   "metadata": {
    "id": "679b12e8"
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = create_dataset(train, train_label, window_size)\n",
    "train_dataset_w = ClassifierDataset(torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).long())\n",
    "\n",
    "test_X, test_Y = create_dataset(test, test_label, window_size)\n",
    "test_dataset_w = ClassifierDataset(torch.from_numpy(test_X).float(), torch.from_numpy(test_Y).long())\n",
    "\n",
    "val_X, val_Y = create_dataset(val, val_label, window_size)\n",
    "val_dataset_w = ClassifierDataset(torch.from_numpy(val_X).float(), torch.from_numpy(val_Y).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5534850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, array, label, index_data, window):\n",
    "        self.index_data = index_data\n",
    "        self.array = array\n",
    "        self.label = label\n",
    "        self.window = window\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file_index = self.index_data[index][0].tolist()\n",
    "        i = self.index_data[index][1]\n",
    "        j = self.index_data[index][2]\n",
    "        if i == j:\n",
    "            x_data = [self.array[file_index][i]] * self.window\n",
    "            y_data = self.label[file_index][i]\n",
    "            X = torch.from_numpy(np.array(x_data).astype(float)).float()\n",
    "            Y = torch.from_numpy(np.array([y_data]).astype(int)).long()\n",
    "            return X, Y\n",
    "        elif j-i < self.window-1:\n",
    "            x_data = [self.array[file_index][i]] * (self.window-j)\n",
    "            x_data.extend(self.array[file_index][(i+1):(j+1)])\n",
    "            x_data = np.array(x_data).astype(float)\n",
    "        else:\n",
    "            x_data = self.array[file_index][i:(j+1)]\n",
    "            k = j-1\n",
    "            while len(x_data) > self.window:\n",
    "                x_data = self.array[file_index][i:(k+1)]\n",
    "                k -= 1\n",
    "        X = torch.from_numpy(x_data).float()\n",
    "        y_data = self.label[file_index][i:(j+1)]\n",
    "        y_mode = mode(y_data)[0][0]\n",
    "        Y = torch.from_numpy(np.array([y_mode])).long()\n",
    "        return X, Y\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.index_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(array_data, label_data, window):\n",
    "    X = []\n",
    "    array_data_len = len(array_data)\n",
    "    for index, array, label in zip(range(array_data_len), array_data, label_data):\n",
    "        l = array.shape[0]\n",
    "        i = 0\n",
    "        while l - i >= window:\n",
    "            j = i + window\n",
    "            index_batch = np.array([index, i, j]).astype(int)\n",
    "            X.append(index_batch)\n",
    "            i += 1\n",
    "    X_batch = torch.from_numpy(np.array(X).astype(int))\n",
    "    return ClassifierDataset(array_data, label_data, X_batch, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e96e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset2(array_data, label_data, window):\n",
    "    X = []\n",
    "    files_num = len(array_data)\n",
    "    for index, array, label in zip(range(files_num), array_data, label_data):\n",
    "        l = array.shape[0]\n",
    "        i = -1\n",
    "        w = [0] * window\n",
    "        while i < l:\n",
    "            i += 1 \n",
    "            w.pop(0)\n",
    "            w.append(i)\n",
    "            index_window = np.array([index, w[0], i]).astype(int)\n",
    "            X.append(index_window)\n",
    "    X_data = torch.from_numpy(np.array(X).astype(int))\n",
    "    return ClassifierDataset(array_data, label_data, X_data, window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i3SiYeWKKX9P",
   "metadata": {
    "id": "i3SiYeWKKX9P"
   },
   "source": [
    "## Weighted Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fGJur35nKc4Y",
   "metadata": {
    "id": "fGJur35nKc4Y"
   },
   "source": [
    "Poiché c'è uno squilibrio di classe, utilizziamo la suddivisione stratificata per creare i nostri set di train, convalida e test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XAC-pLjfKlRB",
   "metadata": {
    "id": "XAC-pLjfKlRB"
   },
   "outputs": [],
   "source": [
    "target_list = []\n",
    "\n",
    "for _, t in train_dataset_w:\n",
    "    target_list.append(t)\n",
    "    \n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zSWgB6_kLDcB",
   "metadata": {
    "id": "zSWgB6_kLDcB"
   },
   "source": [
    "Quindi, otteniamo il conteggio di tutte le classi nel nostro set di allenamento.  Usiamo il reciproco di ogni conteggio per ottenere il suo peso.  Ora che abbiamo calcolato i pesi per ogni classe, possiamo procedere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AfYqQv22LFiY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AfYqQv22LFiY",
    "outputId": "8ffd0976-ef7b-4d4d-f562-9fbe963c3e3c"
   },
   "outputs": [],
   "source": [
    "class_count = [i for i in get_class_distribution(train_label).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTcnQd-qLm_E",
   "metadata": {
    "id": "kTcnQd-qLm_E"
   },
   "source": [
    "WeightedRandomSampler prevede un peso per ogni campione.  Lo usiamo come segue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_k5ysLm-LRoX",
   "metadata": {
    "id": "_k5ysLm-LRoX"
   },
   "outputs": [],
   "source": [
    "class_weights_all = class_weights[target_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jzKuSiX2LuMs",
   "metadata": {
    "id": "jzKuSiX2LuMs"
   },
   "source": [
    "Infine, inizializziamo il nostro WeightedRandomSampler.  Lo chiameremo nel nostro dataloader in seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Kcco4ucL3j7",
   "metadata": {
    "id": "0Kcco4ucL3j7"
   },
   "outputs": [],
   "source": [
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449aca9",
   "metadata": {
    "id": "e449aca9"
   },
   "source": [
    "## Parametri del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404be4ab",
   "metadata": {
    "id": "404be4ab"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "window = 330\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.04\n",
    "NUM_LAYER = 1\n",
    "NUM_HIDDEN = 78\n",
    "NUM_FEATURES = 156\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_array, train_label_array, window)\n",
    "test_dataset = create_dataset2(test_array, test_label_array, window)\n",
    "val_dataset = create_dataset2(val_array, val_label_array, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xIhZwk2cXc4i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIhZwk2cXc4i",
    "outputId": "d0820529-5910-4743-aa2b-5aa0c60f3b26"
   },
   "outputs": [],
   "source": [
    "print(train_dataset.index_data.shape)\n",
    "print(len(train_dataset))\n",
    "print(test_dataset.index_data.shape)\n",
    "print(len(test_dataset))\n",
    "print(val_dataset.index_data.shape)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5701a",
   "metadata": {
    "id": "20e5701a"
   },
   "source": [
    "## Rete Neurale LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EGVAIQMIS685",
   "metadata": {
    "id": "EGVAIQMIS685"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, window, output_size, hidden_layer_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_layer_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.regressor = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is not None:\n",
    "            h0 = hidden[0]\n",
    "            c0 = hidden [1]\n",
    "        else:\n",
    "            h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size).to(device)\n",
    "        h, hn = self.lstm(x, (h0, c0))\n",
    "        h = h.squeeze(1)\n",
    "        h = self.regressor(h[:,-1,:])        \n",
    "        return h, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e18779",
   "metadata": {
    "id": "78e18779"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8781ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a8781ca",
    "outputId": "45a33d5b-405d-4afb-c412-9b1546cbd1ce"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62349e74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62349e74",
    "outputId": "2442ccad-2864-4e12-ddf6-138383e40447"
   },
   "outputs": [],
   "source": [
    "model = LSTM(input_size=NUM_FEATURES, window=window, output_size=NUM_CLASSES, hidden_layer_size=NUM_HIDDEN, num_layers=NUM_LAYER)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e16a8",
   "metadata": {
    "id": "930e16a8"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b5ea5",
   "metadata": {
    "id": "d38b5ea5"
   },
   "source": [
    "Prima di iniziare il nostro addestramento, definiamo una funzione per calcolare la precisione per epoca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77daf61",
   "metadata": {
    "id": "e77daf61"
   },
   "source": [
    "Questa funzione accetta y_pred e y_test come argomenti di input.  Quindi applichiamo log_softmax a y_pred ed estraiamo la classe che ha una probabilità maggiore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0aa302",
   "metadata": {
    "id": "ab0aa302"
   },
   "source": [
    "Successivamente, confrontiamo le classi previste e le classi effettive per calcolare l'accuratezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beef30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return accuracy_score(y_true, y_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ce754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_s(y_pred, y_true):\n",
    "    return f1_score(y_true, y_pred, average=None, zero_division=1, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_data_size:',test_dataset.index_data.size())\n",
    "print('test_label_size:',test_dataset.index_data.size())\n",
    "n_data_size_test = test_dataset.index_data.size()[0]\n",
    "print('n_data_size_test:',n_data_size_test)\n",
    "\n",
    "print('train_data_size:',train_dataset.index_data.size())\n",
    "print('train_label_size:',train_dataset.index_data.size())\n",
    "n_data_size_train = train_dataset.index_data.size()[0]\n",
    "print('n_data_size_train:',n_data_size_train)\n",
    "\n",
    "print('val_data_size:',val_dataset.index_data.size())\n",
    "print('val_label_size:',val_dataset.index_data.size())\n",
    "n_data_size_val = val_dataset.index_data.size()[0]\n",
    "print('n_data_size_val:',n_data_size_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2c253",
   "metadata": {
    "id": "85a2c253"
   },
   "source": [
    "Definiremo anche 2 dizionari che memorizzeranno l'accuratezza/epoca e la perdita/epoca sia per il treno che per i set di convalida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907075c",
   "metadata": {
    "id": "c907075c"
   },
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "f1_score_No_action = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "f1_score_Prendi = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "f1_score_Rilascia = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "f1_score_Premi = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3819558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin training.\")\n",
    "\n",
    "print_every = 5\n",
    "\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    train_epoch_f1_No_action = 0\n",
    "    train_epoch_f1_Prendi = 0\n",
    "    train_epoch_f1_Rilascia = 0\n",
    "    train_epoch_f1_Premi = 0\n",
    "    train_pred_lab = []\n",
    "    train_true_lab = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    hidden_train = None\n",
    "    \n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        y_train_batch = torch.squeeze(y_train_batch)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred, _ = model(X_train_batch, hidden_train)\n",
    "\n",
    "        #h0, c0 = hidden_train\n",
    "        #h0.detach_(), c0.detach_()\n",
    "        #hidden_train = (h0, c0)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_pred_lab.append(y_train_pred)\n",
    "        train_true_lab.append(y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "    y_pred_tags = []\n",
    "    y_true_tags = []\n",
    "    for (p, t) in zip(train_pred_lab, train_true_lab):\n",
    "        _, y_tags = torch.max(p, dim = 1)\n",
    "        y_pred_tags.extend(y_tags.tolist())\n",
    "        y_true_tags.extend(t.tolist())\n",
    "    \n",
    "    train_acc = accuracy(y_pred_tags, y_true_tags)\n",
    "    train_f1 = f1_s(y_pred_tags, y_true_tags)\n",
    "    \n",
    "    train_epoch_acc += train_acc.item()\n",
    "    train_epoch_f1_No_action += train_f1[0].item()\n",
    "    train_epoch_f1_Prendi += train_f1[1].item()\n",
    "    train_epoch_f1_Rilascia += train_f1[2].item()\n",
    "    train_epoch_f1_Premi += train_f1[3].item()\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        \n",
    "        # VALIDATION    \n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "            val_epoch_f1 = 0\n",
    "            val_epoch_f1_No_action = 0\n",
    "            val_epoch_f1_Prendi = 0\n",
    "            val_epoch_f1_Rilascia = 0\n",
    "            val_epoch_f1_Premi = 0\n",
    "            val_pred_lab = []\n",
    "            val_true_lab = []\n",
    "\n",
    "            #hidden_valid = (h0, c0)\n",
    "            hidden_valid = None\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred, _ = model(X_val_batch, hidden_valid)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch[0])\n",
    "                val_pred_lab.append(y_val_pred)\n",
    "                val_true_lab.append(y_val_batch[0])\n",
    "                \n",
    "                val_epoch_loss += val_loss.item()\n",
    "                \n",
    "            y_pred_tags = []\n",
    "            y_true_tags = []\n",
    "            for (p, t) in zip(val_pred_lab, val_true_lab):\n",
    "                _, y_tags = torch.max(p, dim = 1)\n",
    "                y_pred_tags.extend(y_tags.tolist())\n",
    "                y_true_tags.extend(t.tolist())\n",
    "                \n",
    "            val_acc = accuracy(y_pred_tags, y_true_tags)\n",
    "            val_f1 = f1_s(y_pred_tags, y_true_tags)\n",
    "\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            val_epoch_f1_No_action += val_f1[0].item()\n",
    "            val_epoch_f1_Prendi += val_f1[1].item()\n",
    "            val_epoch_f1_Rilascia += val_f1[2].item()\n",
    "            val_epoch_f1_Premi += val_f1[3].item()\n",
    "\n",
    "        loss_stats['train'].append(train_epoch_loss/len(train_loader))        \n",
    "        loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "        accuracy_stats['train'].append(train_epoch_acc)\n",
    "        accuracy_stats['val'].append(val_epoch_acc)\n",
    "        f1_score_No_action['train'].append(train_epoch_f1_No_action)\n",
    "        f1_score_No_action['val'].append(val_epoch_f1_No_action)\n",
    "        f1_score_Prendi['train'].append(train_epoch_f1_Prendi)\n",
    "        f1_score_Prendi['val'].append(val_epoch_f1_Prendi)\n",
    "        f1_score_Rilascia['train'].append(train_epoch_f1_Rilascia)\n",
    "        f1_score_Rilascia['val'].append(val_epoch_f1_Rilascia)\n",
    "        f1_score_Premi['train'].append(train_epoch_f1_Premi)\n",
    "        f1_score_Premi['val'].append(val_epoch_f1_Premi)\n",
    "\n",
    "#         writer.add_scalars(\"Loss/train_val\", {\n",
    "#             \"train\": train_epoch_loss/len(train_loader),\n",
    "#             \"val\": val_epoch_loss/len(val_loader),\n",
    "#         }, e)\n",
    "#         writer.add_scalars(\"Acc/train_val\", {\n",
    "#             \"train\": train_epoch_acc,\n",
    "#             \"val\": val_epoch_acc,\n",
    "#         }, e)\n",
    "#         writer.add_scalars(\"F1-No_action/train_val\", {\n",
    "#             \"train\": train_epoch_f1_No_action,\n",
    "#             \"val\": val_epoch_f1_No_action,\n",
    "#         }, e)\n",
    "#         writer.add_scalars(\"F1-Prendi/train_val\", {\n",
    "#             \"train\": train_epoch_f1_Prendi,\n",
    "#             \"val\": val_epoch_f1_Prendi,\n",
    "#         }, e)\n",
    "#         writer.add_scalars(\"F1-Rilascia/train_val\", {\n",
    "#             \"train\": train_epoch_f1_Rilascia,\n",
    "#             \"val\": val_epoch_f1_Rilascia,\n",
    "#         }, e)\n",
    "#         writer.add_scalars(\"F1-Premi/train_val\", {\n",
    "#             \"train\": train_epoch_f1_Premi,\n",
    "#             \"val\": val_epoch_f1_Premi,\n",
    "#         }, e)\n",
    "#         writer.flush()\n",
    "        print(f'Epoch {e+0:03}:\\\n",
    "              | Train Loss: {train_epoch_loss/len(train_loader)}\\\n",
    "              | Val Loss: {val_epoch_loss/len(val_loader)}\\\n",
    "              | Train Acc: {train_epoch_acc}\\\n",
    "              | Val Acc: {val_epoch_acc}\\\n",
    "              | Train F1-No_action: {train_epoch_f1_No_action}\\\n",
    "              | Val F1-No_action: {val_epoch_f1_No_action}\\\n",
    "              | Train F1-Prendi: {train_epoch_f1_Prendi}\\\n",
    "              | Val F1-Prendi: {val_epoch_f1_Prendi}\\\n",
    "              | Train F1-Rilascia: {train_epoch_f1_Rilascia}\\\n",
    "              | Val F1-Rilascia: {val_epoch_f1_Rilascia}\\\n",
    "              | Train F1-Premi: {train_epoch_f1_Premi}\\\n",
    "              | Val F1-Premi: {val_epoch_f1_Premi}')\n",
    "# writer.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53d3eb",
   "metadata": {
    "id": "bf53d3eb"
   },
   "source": [
    "## Visualizzazione Loss e Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362aee",
   "metadata": {
    "id": "21362aee"
   },
   "source": [
    "Per tracciare i grafici delle linee di perdita e accuratezza, creiamo nuovamente un dataframe dai dizionari precision_stats e loss_stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f2836",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "d42f2836",
    "outputId": "7e74ec3c-472f-43b6-a636-703dd05a1d13"
   },
   "outputs": [],
   "source": [
    "# Create dataframes\n",
    "train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "train_val_f1_No_action = pd.DataFrame.from_dict(f1_score_No_action).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "train_val_f1_Prendi = pd.DataFrame.from_dict(f1_score_Prendi).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "train_val_f1_Rilascia = pd.DataFrame.from_dict(f1_score_Rilascia).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "train_val_f1_Premi = pd.DataFrame.from_dict(f1_score_Premi).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "\n",
    "# Plot the dataframes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
    "\n",
    "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
    "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BXoi7gM63d_X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "BXoi7gM63d_X",
    "outputId": "a6d6e802-7f71-4994-b61d-b497b90fbc36"
   },
   "outputs": [],
   "source": [
    "# Plot the dataframes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
    "\n",
    "sns.lineplot(data=train_val_f1_No_action, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[0]).set_title('Train-Val F1-No_action/Epoch')\n",
    "sns.lineplot(data=train_val_f1_Prendi, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val F1-Prendi/Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZOfSJCpy3ed6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "ZOfSJCpy3ed6",
    "outputId": "0547a758-cbbc-44ca-c49a-63ee035b05dd"
   },
   "outputs": [],
   "source": [
    "# Plot the dataframes\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
    "\n",
    "sns.lineplot(data=train_val_f1_Rilascia, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[0]).set_title('Train-Val F1-Rilascia/Epoch')\n",
    "sns.lineplot(data=train_val_f1_Premi, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val F1-Premi/Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0b337",
   "metadata": {
    "id": "bac0b337"
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84238398",
   "metadata": {
    "id": "84238398"
   },
   "source": [
    "Al termine dell'addestramento, dobbiamo testare come si è comportato il nostro modello.  Nota che abbiamo usato model.eval() prima di eseguire il nostro codice di test.  Per dire a PyTorch che non vogliamo eseguire la retropropagazione durante l'inferenza, usiamo torch.no_grad(), proprio come abbiamo fatto per il ciclo di convalida sopra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0fd7f",
   "metadata": {
    "id": "d2f0fd7f"
   },
   "source": [
    "Iniziamo definendo un elenco che conterrà le nostre previsioni.  Quindi eseguiamo il ciclo dei nostri batch utilizzando test_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred, _ = model(X_batch)\n",
    "        _, y_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_tags.cpu().tolist())\n",
    "        y_true_list.append(y_batch[0][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred_list) == len(y_true_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5507ef3",
   "metadata": {
    "id": "a5507ef3"
   },
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3aa387",
   "metadata": {
    "id": "8a3aa387"
   },
   "source": [
    "Creiamo un dataframe dalla matrice di confusione e lo tracciamo come una mappa di calore utilizzando la libreria Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bfae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "6a1bfae3",
    "outputId": "dae0b352-64fc-40d4-8d1c-bdf972a56777"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_true_list, y_pred_list, normalize='true')).rename(columns=idx2class, index=idx2class)\n",
    "sns.heatmap(confusion_matrix_df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b39741",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94b39741",
    "outputId": "787851a7-7d1f-4e53-cb73-5ad5f2c17dad"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-Copy1.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0135b41a3a4340f99d7f448ce18cd460": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2142be525ab64432b2de9edb80a5ac42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "350aab8898b649a0ae733a5ac572f0db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bfd0cda32ad4b1f9a146a139a28bf6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b809dd46be0943878e4494089332b7be",
      "placeholder": "​",
      "style": "IPY_MODEL_0135b41a3a4340f99d7f448ce18cd460",
      "value": " 50/50 [10:18&lt;00:00, 12.38s/it]"
     }
    },
    "7802ba29b3e5429e8ee1ed115d1ca93b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c73e79e30252416985595da1660544a1",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2142be525ab64432b2de9edb80a5ac42",
      "value": 50
     }
    },
    "9a9d334287f54a8d84800ed57d650997": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7802ba29b3e5429e8ee1ed115d1ca93b",
       "IPY_MODEL_5bfd0cda32ad4b1f9a146a139a28bf6d"
      ],
      "layout": "IPY_MODEL_350aab8898b649a0ae733a5ac572f0db"
     }
    },
    "b809dd46be0943878e4494089332b7be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c73e79e30252416985595da1660544a1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
